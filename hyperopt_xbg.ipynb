{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dealing with imbalanced classes. \n\n* using a combination of **hyperopt** and the **XGBoost-Classifier**. To see my experiments with a Neural Network Autoencoder click here. \n\n\n* Using the Kaggle **Credit Card Fraud Detection** dataset. \n\n\n* Quoting from Kaggle: \n\n    - The datasets contains transactions made by credit cards in September 2013 by european cardholders.This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n    \n    - Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n    \n    - Features V1-V28 are anonymized. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount.\n    \n    \n* XGBoost \n\n    - is known for its excellent speed & performance (operations parallizable) on a variety on Machine Learning problems. \n    - Is an Ensemble algoritm. It uses CART decision trees as base learners. \n    \n    - It is preferred for use when we have a large number of training samples. Number of training samples must be greater than the number of features. \n    \n    - The complete parameter list can be found at https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst\n    \n    \n* Hyperopt\n\n    - Manual search, grid search and random search are common algorithms used for hyperparameter tuning. \n    - All these previous methods are un-informed: ie. they scan through the entire/randomly selected set of hyperparameters to select the best one. \n    - Hyperopt is a Bayasian optimization algorithm, it is classified as an 'informed search' algorithm as the score from the previous round of search, 'informs' the choice of a better set of hyperparameters. \n \n* Another extremely cool advantage of hyperopt is: \n    - There are a lot of machine learning algorithms. And many different configuration for preprocessing/ feature engineering. Hyperopt allows us to use an algorithm to search this large set of other algorithms for the best, without additional input on our part. \n    - The 'algo' parameter is customizable. I will only use the inbuilt tpe.suggest algo. It is known to work well for most use cases. \n    - My dataset does not need much pre-processing. \n    - So I will only try out hyperparameter tuning for XGBoost today. "},{"metadata":{},"cell_type":"markdown","source":"# Imports & read file"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport plotly_express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve\nfrom sklearn.model_selection import train_test_split\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\nimport pickle\nimport xgboost as xgb\n\n%matplotlib inline\nsns.set(style='ticks')\npd.options.display.max_columns = 500\npd.options.display.max_rows = 500\n\ndata = pd.read_csv(\"../input/creditcardfraud/creditcard.csv\")","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore data"},{"metadata":{"trusted":false},"cell_type":"code","source":"data.info()\ndata.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 30 feature columns in dataset. \n    - We don't know what V1-V26 are, but we know they have been scaled (boxplots show a similar range of values, to confirm).\n    - Additionally,  'Time' in seconds, over a period of 2 days, is available. \n    - Majority of transactions are of a smaller 'Amount', mean is USD 88, range is ~ USD 0 - USD 25,691 range.\n    \n    \n* No null values in dataset. \n\n\n* Imbalanced classes can be seen below. "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Target contains imbalanced classes\ndata.Class.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Boxplot of variables V1 - V28\nplt.figure(figsize=(20,5))\nsns.boxplot(data=data.drop(['Time', 'Amount', 'Class'], axis=1))\nplt.xticks(rotation=90)\nplt.title('Boxplots of features V1 - V26')\nplt.xlabel('Feature Name')\nplt.ylabel('Value')\nplt.show()\n\n#datelist=pd.date_range(\"00:00:00\", \"23:59:59\", freq=\"S\")\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,5))\nax1.scatter(data.index, data['Time']/(60*60))\nax1.set_xlabel('Index')\nax1.set_ylabel('Time feature (in hours)')\nax1.set_title('Scatterplot of Time feature')\nax2.boxplot(data['Amount'])\nax2.set_xlabel('Amount')\nax2.set_title('Boxplot of Amount feature')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling 'Time' and 'Amount'\nfrom sklearn.preprocessing import StandardScaler\nstd_scaler = StandardScaler()\n\ndata['scaled_amount'] = std_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\ndata['scaled_time'] = std_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n\ndata.drop(['Time','Amount'], axis=1, inplace=True)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_rest, y_train, y_rest= train_test_split(data.drop('Class', axis=1), data.Class, \n                                                   test_size=0.5, random_state=123)\nX_hold, X_test, y_hold, y_test= train_test_split(X_rest, y_rest, \n                                                   test_size=0.5, random_state=123)","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models\n\n## XGBoost"},{"metadata":{},"cell_type":"markdown","source":"### Baseline model using XGBClassifier, using common test_train_split"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(n_estimators=50, seed=123)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_hold)\naccuracy = accuracy_score(y_hold, y_pred)\nprint(\"accuracy: %f\" % (accuracy))\nprint('As 99.828% of the data is class 0, the 99.927% heavily biased accuracy does not tell us much about the quality of our model .')\n\nf1 = f1_score(y_hold, y_pred)\nprint(\"f1 score is a better metric: %f\" % (f1))\n\nprint(confusion_matrix(y_hold, y_pred))\nprint(classification_report(y_hold, y_pred))","execution_count":25,"outputs":[{"output_type":"stream","text":"accuracy: 0.999396\nAs 99.828% of the data is class 0, the 99.927% heavily biased accuracy does not tell us much about the quality of our model .\nf1 score is a better metric: 0.833977\n[[71051    19]\n [   24   108]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     71070\n           1       0.85      0.82      0.83       132\n\n    accuracy                           1.00     71202\n   macro avg       0.93      0.91      0.92     71202\nweighted avg       1.00      1.00      1.00     71202\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Sampling & Models : Decreases performance\n\n* **OVERSAMPLING OR UNDERSAMPLING SHOULD ONLY BE APPLIED TO TRAIN.** "},{"metadata":{},"cell_type":"markdown","source":"* #### Undersampling : Precision very low, Recall is high. "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_0=y_train[y_train==0]\ny_1=y_train[y_train==1]\ny_0_under=y_0.sample(n=len(y_1), random_state=123)\ny_under=pd.concat([y_0_under,y_1]).sample(frac=1, random_state=123)\nX_under=X_train.reindex(y_under.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target is no longer imbalanced. \npd.DataFrame(y_under)['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target is no longer imbalanced. \npd.DataFrame(y_under)['Class'].value_counts()\n\nclf = xgb.XGBClassifier(n_estimators=50, seed=123)\nclf.fit(X_under, y_under)\ny_pred = clf.predict(X_hold)\nprint(confusion_matrix(y_hold, y_pred))\nprint(classification_report(y_hold, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Oversampling : Precision very low, Recall high"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler, SMOTE\nmethod = RandomOverSampler()\nX_over, y_over = method.fit_resample(X_train,y_train)\nX_over = pd.DataFrame(X_over, columns=['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'scaled_amount',\n       'scaled_time'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target is no longer imbalanced. \npd.DataFrame(y_over)['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(n_estimators=50, seed=123)\nclf.fit(X_over, y_over)\ny_pred = clf.predict(X_hold)\nprint(confusion_matrix(y_hold, y_pred))\nprint(classification_report(y_hold, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Synthetic Minority Oversampling: Precision very low, Recall high.\n\n* SMOTE is an improved version of Random Oversampling. But this is useful only when all undersampled cases are similar."},{"metadata":{"trusted":true},"cell_type":"code","source":"method = SMOTE()\nX_over, y_over = method.fit_resample(X_train,y_train)\nX_over = pd.DataFrame(X_over, columns=['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'scaled_amount',\n       'scaled_time'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target is no longer imbalanced. \npd.DataFrame(y_over)['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(n_estimators=50, seed=123)\nclf.fit(X_over, y_over)\ny_pred = clf.predict(X_hold)\nprint(confusion_matrix(y_hold, y_pred))\nprint(classification_report(y_hold, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vanilla XGBoostClassifier does significantly better on unaltered data. Under/oversampling techniques decrease performance. \n\n* This is an interesting result. \n\n* Random Forests and Logistic Regression methods are commonly used for imbalanced class data. These do well with varius sampling strategies. Autoencoder Neural Network work very well to model imbalanced classes. \n\n* XGBoostClassifier does better with the original data. It is faster than most ML algorithms AND needs less preprocessing! \n\n* Let's improve out XGBoostClassifier through hyperparameter tuning. "},{"metadata":{},"cell_type":"markdown","source":"### Using hyperopt for XGBoostClassifier hyperparameter tuning \n\n* Parameter spaces can be built from manually entering values or selecting from one of these distrbutions.\n    - hp.choice(label, options) — Returns one of the options, which should be a list or tuple.\n    - hp.randint(label, upper) — Returns a random integer between the range (0, upper).\n    - hp.uniform(label, low, high) — Returns a value uniformly between low and high.\n    - hp.quniform(label, low, high, q) — Returns a value round(uniform(low, high) / q) * q, i.e it rounds the decimal values and returns an integer\n    - hp.normal(label, mean, std) — Returns a real value that’s normally-distributed with mean and standard deviation sigma.\n    \n    \n* The function optimized by hyperopt always minimizes - so we have to return 1-f1_score, in order to maximize this metric of our choice. \n\n\n* An odd feature: parameter names have to be provided twice in the parameter selection space. \n\n\n* XGBoost uses a special matrix type called a DMatrix. xgb.XGBClassifier automatically groups the data in a DMatrix, but when using cross validation or more complicated code, we must explicitly convert data to a DMatrix first. \n\n\n* the Trials() function stores data as the hyperopt algorithm progresses. It allows us to learn a few details about the internal working of the hyperopt algorithm. Running the Trials() function is optional. \n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Below code has been commented out because it takes time a long time (~30 mins with. 8 cores, GPU acceleration) \n# to run on every commit. So it was run once and commented out.\n# defining the space for hyperparameter tuning\nspace={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),'gamma': hp.uniform ('gamma', 1,9),\n       'reg_alpha' : hp.quniform('reg_alpha', 1,180,1),\n       'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n       'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n       'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n       }\n\ndef hyperparameter_tuning(space):\n    clf=xgb.XGBClassifier(max_depth = int(space['max_depth']), gamma = space['gamma'],\n                         reg_alpha = int(space['reg_alpha']),min_child_weight=space['min_child_weight'],\n                         colsample_bytree=space['colsample_bytree'],eta= 0.8, nthread=-1, \n                         scale_pos_weight = np.sqrt(sum(y_train==0)/sum(y_train==1)),\n                          n_estimators=50, random_state=123)\n    evaluation = [( X_train, y_train), ( X_hold, y_hold)]\n    \n    clf.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"rmse\",\n            early_stopping_rounds=10,verbose=False)\n\n    y_pred = clf.predict(X_hold)\n    recall = recall_score(y_hold, y_pred)\n    print(classification_report(y_hold, y_pred))\n    # We want to ensure that every fraud case is reported. \n    # Even if we tradeoff a large number of false positives. \n    # So we will maximize recall (ie. minimize 1-recall)\n    print (\"Recall:\", recall)\n    return {'loss': 1-recall, 'status': STATUS_OK }\n\n\n# run the hyper paramter tuning\ntrials = Trials()\n\nbest = fmin(fn=hyperparameter_tuning,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=100)\n\nprint (best)","execution_count":null,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support  \n\n           0       1.00      1.00      1.00     71070\n           1       0.70      0.86      0.77       132\n\n    accuracy                           1.00     71202\n   macro avg       0.85      0.93      0.89     71202\nweighted avg       1.00      1.00      1.00     71202\n\nRecall:                                                \n0.8636363636363636                                     \n  1%|          | 1/100 [00:47<1:18:27, 47.55s/trial, best loss: 0.13636363636363635]","name":"stdout"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Best output from hyperparameter tuning.\n\"\"\"\n              precision    recall  f1-score   support                              \n\n           0       1.00      1.00      1.00    142147\n           1       0.90      0.80      0.85       257\n\n    accuracy                           1.00    142404\n   macro avg       0.95      0.90      0.92    142404\nweighted avg       1.00      1.00      1.00    142404\n\nRecall:                                                                            \n0.8015564202334631                                                                 \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":false},"cell_type":"code","source":"# Model with best parameters (80% recall, 85% f1 score)\nclf = xgb.XGBClassifier(colsample_bytree= 0.7356225471858802,\n gamma = 5.0244632204209765,\n max_depth=15,\n min_child_weight=4,\n reg_alpha = 1,\n reg_lambda = 0.49906388627478493, eta= 0.8, nthread=-1, n_estimators=50, seed=123)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_hold)\nprint(confusion_matrix(y_hold, y_pred))\nprint(classification_report(y_hold, y_pred))\n\n# Default parameters in Vanilla baseline model (80% similar recall, 82% lower f1 score)\n\"\"\"\ncolsample_bytree= 1,\n gamma = 0,\n max_depth = 6,\n min_child_weight = 1,\n reg_alpha = 0,\n reg_lambda = 1\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Even though the baseline XGBoostClassifier has a similar recall and few percent lower f1 score, the tuned model is much better because: \n    - it has alpha & lambda regularization as well as a higher gamma value (higher gamma = greater gain threshold for maintaining nodes, and thus, more pruning). \n    - 7.36/10th fraction of the features are subsampled in the tuned model. We thus make the model run faster (and introduce regularization) without losing out on performance. \n    - The tuned model builds deeper trees (max depth 6 vs 15). A greater number of interactions can be captured by the tuned model. \n    - A higher weight threshold is used to allow/deny subdivision of node, in the tuned model. \n    \n    \n* Further tuning with a lower learning rate (eta) can be done to narrow down on a better model. \n\n* Finally. the precision-recall threshold should be changed to ensure better recall, if possible. We see in the current precision - recall curve, there is a sharp drop in precision if we require >80% recall. But we also see that the mean amount of 492 fraud transactions is higher than the mean of 284315 non-fraud transactions. Considering the higher losses, the bank may want to increase recall to ~99% even if it means more calls to customers to check on legitimate transctions flagged as fraud.       "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot precision recall curve for current best model. \nprecisions, recalls, thresholds = precision_recall_curve(y_hold, y_pred)\nplt.plot(recalls, precisions, \"b-\", linewidth=2)\nplt.xlabel(\"Recall\", fontsize=16)\nplt.ylabel(\"Precision\", fontsize=16)\nplt.axis([0, 1, 0, 1])\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot mean Amounts of non-fraud (label 0) and fraud (label 1) transactions\nsns.catplot(x='Class', y='Amount', data=data, kind='bar', ci=None)\nplt.title('Mean Amounts for non-fraud (label 0) and fraud (label 1) transactions' )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}